{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "sacred-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "nltk.download('state_union')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "noted-groove",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "mysterious-registrar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "talented-accident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('happy.a.01'), Synset('felicitous.s.02'), Synset('glad.s.02'), Synset('happy.s.04')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"Happy\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "broad-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bravely Default II is a classic RPG that feels a little too familiar\n",
      "Tree('S', [('input_ids', 'NNS'), ('token_type_ids', 'NNS'), ('attention_mask', 'NN')])\n"
     ]
    }
   ],
   "source": [
    "# tokenized = tokenizer.tokenize(seq[100])\n",
    "# for i in tokenized:\n",
    "#     words = tokenizer(i)\n",
    "#     tagged = nltk.pos_tag(words)\n",
    "#     namedEnt = nltk.ne_chunk(tagged)\n",
    "# # https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "# print(seq[100])\n",
    "# for n in namedEnt:\n",
    "#     if type(n) == Tree:\n",
    "#         print(\" \".join([token for token,pos in n.leaves()]))\n",
    "# print(namedEnt.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "tracked-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"scraped.csv\")\n",
    "df.drop([\"Unnamed: 0\"],inplace=True,axis=1)\n",
    "seq = df[\"Headlines\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "frank-progress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge approves $650 million Facebook privacy settlement over facial recognition feature\n"
     ]
    }
   ],
   "source": [
    "sequence = seq[9]\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer()\n",
    "tokenized = custom_sent_tokenizer.tokenize(sequence)\n",
    "\n",
    "for i in tokenized:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    namedEnt = nltk.ne_chunk(tagged)\n",
    "# https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "print(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fifteen-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = []\n",
    "for n in namedEnt:\n",
    "    if type(n) == Tree:\n",
    "        for token,pos in n.leaves():\n",
    "            if pos in [\"NNP\",'NN']:\n",
    "                print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "respective-clarity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Judge', 'NNP')\n",
      "('Facebook', 'NNP')\n",
      "('privacy', 'NN')\n",
      "('settlement', 'NN')\n",
      "('recognition', 'NN')\n",
      "('feature', 'NN')\n"
     ]
    }
   ],
   "source": [
    "l = [\"NN\",\"NNP\"]\n",
    "for tags in tagged:\n",
    "    if tags[1] in l:\n",
    "        print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "suffering-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text):\n",
    "    text= \" \".join([word for word in list(text.split(\" \")) if word not in stopwords.words(\"english\")])\n",
    "    chunked = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token,pos in i.leaves()]))\n",
    "        if current_chunk:\n",
    "            named_entity = \",\".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "failing-plenty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Judge approves $650 million Facebook privacy settlement facial recognition feature'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "revolutionary-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing(text):\n",
    "#     filtered= \" \".join([word for word in list(text.split(\" \")) if word not in stopwords.words(\"english\")])\n",
    "#     return filtered\n",
    "\n",
    "def NER(text):\n",
    "    text= \" \".join([word for word in list(text.split(\" \")) if word not in stopwords.words(\"english\")])\n",
    "    chunked = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token,pos in i.leaves()]))\n",
    "        if current_chunk:\n",
    "            named_entity = \",\".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "recreational-rainbow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Steam']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = NER(seq[110])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "noticed-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "stone-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = r\"C:\\Program Files\\Java\\jre1.8.0_281\\bin\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "rough-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = StanfordNERTagger(r'C:\\Users\\astar\\Stock market tutorials\\Stanford NER\\english.all.3class.distsim.crf.ser.gz',\n",
    "                      r'C:\\Users\\astar\\Stock market tutorials\\Stanford NER\\stanford-ner.jar',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = seq[9]\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-nudist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
