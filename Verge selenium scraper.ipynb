{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "terminal-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "#from webdriver_manager.chrome import ChromeDriverManager \n",
    "#driver=webdriver.Chrome(ChromeDriverManager().install()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "incorporated-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrolling_func(wait,driver):\n",
    "    SCROLL_PAUSE_TIME = 1.0\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.p-button')))\n",
    "        load_button = driver.find_element_by_css_selector('.p-button')\n",
    "        load_button.click()\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "passive-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(years,months):\n",
    "    base_url = \"https://www.theverge.com/archives\" \n",
    "    urls = []\n",
    "    zipped = zip(years,months)\n",
    "    for year,month in zipped:\n",
    "        url =  base_url+\"/\"+year+\"/\"+month\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "spread-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Scraper(threading.Thread):\n",
    "#     def __init__(self, threadId, name, url,soup):\n",
    "#         threading.Thread.__init__(self)\n",
    "#         self.name = name\n",
    "#         self.id = threadId\n",
    "#         self.url = url\n",
    "\n",
    "#     def run(self):\n",
    "#         titles = soup.find_all(\"h2\",class_=\"c-entry-box--compact__title\")\n",
    "#         dates = soup.find_all(\"time\",class_=\"c-byline__item\")\n",
    "#         return titles,dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "swedish-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(years,months):\n",
    "    CHROME_PATH = r\"C:\\Users\\astar\\Stock market tutorials\\chromedriver_win64\\chromedriver.exe\"\n",
    "    driver = webdriver.Chrome(CHROME_PATH)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    urls = parse_dates(years,months)\n",
    "    \n",
    "    headlines = []\n",
    "    dates = []\n",
    "    links = []\n",
    "\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        done=True\n",
    "     \n",
    "        \n",
    "        while done:\n",
    "            try:\n",
    "                wait = WebDriverWait(driver,100)\n",
    "                scrolling_func(wait,driver)  \n",
    "            except:\n",
    "                done=False\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        #https://stackoverflow.com/questions/5041008/how-to-find-elements-by-class\n",
    "        #https://stackoverflow.com/questions/42732958/python-parallel-execution-with-selenium\n",
    "        #https://stackoverflow.com/questions/44245451/how-to-scrape-multiple-html-page-in-parallel-with-beautifulsoup-in-python\n",
    "        for title in soup.find_all(\"h2\",class_=\"c-entry-box--compact__title\"):\n",
    "            headlines.append(title.text)\n",
    "            s = str(title)[str(title).find(\"href\")+5:]\n",
    "            links.append(s[:s.find(\">\")])\n",
    "        for date in soup.find_all(\"time\",class_=\"c-byline__item\"):\n",
    "            dates.append(date.text.strip())\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "    print(len(headlines),len(dates))    \n",
    "\n",
    "    assert len(headlines)==len(dates), f\"Different lengths of headlines {len(headlines)} and date {len(dates)}\"\n",
    "    data = {\"Headlines\":headlines,\"Dates\":dates,\"Links\":links}\n",
    "    df = pd.DataFrame(data)\n",
    "    driver.quit()    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scraper([\"2021\",\"2021\"],[\"2\",\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "freelance-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"https://www.theverge.com/2021/3/15/22332624/bird-europe-investment-scooter-expansion-new-hires\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib\n",
    "url = \"https://www.theverge.com/archives/2021/3\"\n",
    "url = requests.get(url)\n",
    "soup = BeautifulSoup(url.text,'lxml')\n",
    "titles = []\n",
    "for title in soup.find_all(\"h2\",class_=\"c-entry-box--compact__title\"):\n",
    "    s = str(title)[str(title).find(\"href\")+5:]\n",
    "    print(s[:s.find(\">\")])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-statistics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
